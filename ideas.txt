

mass process all task files
- to see if current rules solve any

auto-generation of tests
- add

ROUTINES

test for symmetries:
- left-right
- up-down
- across diagonals
- translational
- rot: 90, 180, 270
- scaling:  1/2x, 2x, 3x
make monochrome  (all colors to blue == 1)
- this will allow for testing exact equality of matrices modulo color
flatten to string
- on transpose, too;  maybe use regexes to pattern match, then
interleave flattening to help find patterns
percent black, other colors
enclosed areas

WILD

genetic algo which creates regexes


STRATEGIES

divide problems into types

All inputs have the same grid size
All outputs have the same grid size

- maintain grid size
- increase grid size
-- only in x
-- only in y
-- in x and y
- decrease grid size
-- only in x
-- only in y
-- in x and y

# of colors
- increase
- decrease


TASK QUESTIONS

007bbfb7
- is the input scaled up an integer factor?
- are the same colors used in the input and output?
- are there exact copies of the input in the output?
-- assuming yes:  how many copies?  where are they?
-- the hard question:  why are the copies where they are?
--  hard to encode answer: because they are in the non-background color cells in the input



BRAINSTORM

evaluate situation  -> take action(s)
how to percolate evaluation upwards?

there are levels of "evaluation"  (level is a vague word)
- eg:  "color of a cell" is much lower than  "are input and output grids the same?"
- lower levels may influence upper levels, but upper levels may influence lower levels
--  it isn't a strict hierarchy, perhaps

there are kinds of evaluation
- yes/no (binary) properties:  same grid size, same colors
- numbers:  scaling factors, # of colors
- numerical ranges:
- one of a set:  shapes

eval/action of all training samples in a task _taken together_ has to constrain the eval/action of the test

could these be reduced to finding string transformations between input -> output?
--  JSON.stringify(input)
--  pipes and numbers

we would like for the evaluations and actions to be explainable


CAPABILITIES NEEDED

aggregation
- what tasks have a given property?
-- exanples:
---  How many tasks have input and outputs with the same dimensions?  Which?
- answers question:  "how important is this evaluation "
- implementation details
-- dictionary of dictionaries of dictionaries
---  {
         task_id:  {
              sample1:  { solved: false,  prop1:  1,  prop2: 2 , ... }
              sample2:  { solved: false,  prop1:  1,  prop2: 2 , ... }
                  ...
              test:  {  prop1:  1,  prop2: 2 , ... }
                   },
         task_id:  {
              sample1:  {  prop1:  1,  prop2: 2 , ... }
              sample2:  {  prop1:  1,  prop2: 2 , ... }
                  ...
              test:  {  prop1:  1,  prop2: 2 , ... }
                   }
     }
-- pass structure in from test_generator;  each test

Another alternative:
use Jest Globals
- describe.each
- then after.all to aggregate
- maybe use after each for aggregating at the individual task level


a way of determining the actions to take given a set of evaluations

meta: code which develops new evaluations (and actions) on the fly
- example: comparing grid sizes
